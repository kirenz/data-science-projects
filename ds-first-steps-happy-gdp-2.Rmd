---
title: "First steps in Data Science"
subtitle: "Does money make people happier?"
author: "Prof. Dr. Jan Kirenz, HdM Stuttgart"
output:
 html_document:
  code_download: true
  fig_height: 4
  fig_width: 4
  highlight: tango
  number_sections: yes
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  theme: paper
  df_print: paged
---


```{css, echo=FALSE}

/* css code to change the look of the HTML-output */
  
h1 {
  color: #D0313C;
  font-size: 200%;
  }
h2 {
  color: #D0313C;
  font-size: 150%;
  }
h3 {
  font-size: 120%;
  font-weight: bold;
  }
h4 {
  color: rgb(139, 142, 150);
  font-size: 100%;
  font-weight: bold;
  }

```



```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


# Setup

```{r}

# Load packages
library(tidyverse) # collection of important data science packages
library(tidymodels)  # collection of packages for modeling and ml
library(janitor) # data preparation package
library(kknn) # K-nearest neighbor model

```

# Data import

```{r}

# Load the data from GitHub
LINK = "https://raw.githubusercontent.com/kirenz/datasets/master/oecd_gdp.csv"
df <- read_csv(LINK)

# Take a look at the data
glimpse(df)

```

# Data preparation

```{r fig.width=4, fig.height=4}

# Change column names
df <-  clean_names(df)

# Visualize the data
ggplot(df, aes(x=gdp_per_capita, 
               y=life_satisfaction)) +
  geom_point() +
  theme_classic()

```


# Model selection

## Linear regression model

```{r}

lm_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode(mode = "regression") 

```

## K-nearest neighbor

```{r}

knn_mod <- 
  nearest_neighbor(neighbors = 3) %>% 
  set_engine("kknn") %>% 
  set_mode("regression") 

```


# Data splitting

```{r}
set.seed(123)

df_split <- initial_split(df) 
df_train <- training(df_split) 
df_test <- testing(df_split)

```


# Model training 

## K-fold cross-validation

```{r}
set.seed(123)

cv_folds <- vfold_cv(df_train, v = 5)

```

## Linear regression

```{r}

lm_fit <- 
  lm_mod %>% 
  fit_resamples(life_satisfaction ~ gdp_per_capita, 
                resamples = cv_folds)

```

Model summary:

```{r}

# Performance measures for every fold
collect_metrics(lm_fit, summarize = FALSE)

# Average performance accross all folds
collect_metrics(lm_fit, summarize = TRUE)

```


## K-nearest neighbor

```{r}

knn_fit <- 
  knn_mod %>% 
  fit_resamples(life_satisfaction ~ gdp_per_capita, 
                resamples = cv_folds)

```

Model summary

```{r}

# Performance measures for every fold
collect_metrics(knn_fit, summarize = FALSE)

# Average performance accross all folds
collect_metrics(knn_fit, summarize = TRUE)

```


# Train final model

Train best model with all training data:

```{r}

last_lm_fit <- 
  lm_mod %>% 
  fit(life_satisfaction ~ gdp_per_capita, 
                data = df_train)

```

Model coefficients:

```{r}

tidy(last_lm_fit)

```

# Model testing

Predictions:

```{r}

last_lm_pred <-
  last_lm_fit %>%
  predict(new_data = df_test) %>%
  mutate(y_truth = df_test$life_satisfaction)

```

Root mean squared error:

```{r}

rmse(truth = y_truth, 
     estimate = .pred, 
     last_lm_pred) 

```

More performance measures:

```{r}

glance(last_lm_fit$fit)

```

Make a prediction for a new GDP value:

```{r}

X_new <-  tibble(gdp_per_capita = c(50000))

```

```{r}

(predict(last_lm_fit, new_data = X_new))

```


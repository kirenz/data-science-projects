---
title: "Data Science Lifecycle"
subtitle: "Does money make people happier?"
author: "Prof. Dr. Jan Kirenz, HdM Stuttgart"
output:
 html_document:
  code_download: true
  fig_height: 4
  fig_width: 4
  highlight: tango
  number_sections: yes
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  theme: paper
  df_print: paged
---


```{css, echo=FALSE}

/* css code to change the look of the HTML-output */
  
h1 {
  color: #D0313C;
  font-size: 200%;
  }
h2 {
  color: #D0313C;
  font-size: 150%;
  }
h3 {
  font-size: 120%;
  font-weight: bold;
  }
h4 {
  color: rgb(139, 142, 150);
  font-size: 100%;
  font-weight: bold;
  }

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


# Setup

Load packages

```{r}

# Load packages
library(tidyverse) # collection of important data science packages
library(tidymodels)  # collection of packages for modeling and ml
library(janitor) # data preparation package
library(kknn) # K-nearest neighbor model
library(skimr) # to obtain tidy data summaries
library(corrplot) # to study corrleations
library(GGally) # to create combined plots
library(workflows) # to create workflows

```

# Business understanding

Not covered in detail in this notebook. Here are the main points:

* We want a model that predicts life satisfaction of a country as a numerical value.
* We are only able to use GDP per capita as a predictor variable.
* We measure model performance with RMSE.

# Data understanding

## Import data

Load the data from GitHub

```{r}

# Create the link
LINK = "https://raw.githubusercontent.com/kirenz/datasets/master/oecd_gdp.csv"

# Import data
df <- read_csv(LINK)

```

## Data structure

```{r}

# Take a look at the data
glimpse(df)

# Change column names
df <- clean_names(df)

glimpse(df)

```

## Data splitting

Create training and test data:

```{r}
set.seed(123)

df_split <- initial_split(df) 
df_train <- training(df_split) 
df_test <- testing(df_split)

```


## Data exploration

### Copy data

Create a copy of the training data for exploration:

```{r}

df_expl <- df_train

```

### Study attributes

Study each attribute and it's characteristics:

```{r}

# Data overview
df_expl %>% skim() 

```

### Visualize data

Visualize the data.

#### Scatterplot

```{r fig.width=4, fig.height=4}

ggplot(df_expl, aes(x = gdp_per_capita, 
               y = life_satisfaction)) +
  geom_point() +
  theme_classic()

```

#### Histogram

```{r fig.width=4, fig.height=4}

ggplot(df_expl, aes(x = gdp_per_capita)) +
  geom_histogram(bins = 15) +
  geom_density() +
  theme_classic()

```

```{r fig.width=4, fig.height=4}

ggplot(df_expl, aes(x = life_satisfaction)) +
  geom_histogram(bins = 15) +
  theme_classic()

```

#### Boxplot

```{r fig.width=4, fig.height=4}

ggplot(df_expl, aes(x = "", y = gdp_per_capita)) +
  geom_boxplot() +
  theme_classic()

```


```{r fig.width=4, fig.height=4}

ggplot(df_expl, aes(x = "", y = life_satisfaction)) +
  geom_boxplot() +
  theme_classic()

```

#### Combined plots

Create combined plots with GGally.


```{r}

df_expl %>% 
  select(-country) %>% 
  ggpairs()

```



### Correlations

Study correlations:

```{r}
cor <- 
  df_expl %>% 
  select(-country) %>% 
  cor()

cor

```

Correlation plot:

```{r}

corrplot(cor, type = "upper")

```

Correlations with statistical significance:

```{r}

cor.test(df_expl$gdp_per_capita, df_expl$life_satisfaction, 
         method = "pearson")

```


# Data preparation

We don't cover this topic (feature engineering) in this example.  

# Modeling

We want to use three different regression models:

1. simple linear regression.
2. natural spline in conjunction with a linear regression model,
3. K-nearest-neighbour model.

We want to use k-fold cross-validation to identify the best model.

## K-fold cross-validation

Prepare 5-fold cross-validation for model evaluation:

```{r}
set.seed(123)

cv_folds <- vfold_cv(df_train, v = 5)

```

## Linear regression model

### Specify model

Specify the model:

```{r}

lm_spec <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode(mode = "regression") 

```

### Train model

Train the model with k-fold cross-validation:

```{r}

lm_fit <-
  lm_spec %>% 
  fit_resamples(life_satisfaction ~ gdp_per_capita, 
      resamples = cv_folds)

```


### Model evaluation

Obtain model summary. Average performance accross all folds:

```{r}

collect_metrics(lm_fit, summarize = TRUE)

```

Performance measures for every fold


```{r}

collect_metrics(lm_fit, summarize = FALSE)

```


### Plot model

Visualize the model:

```{r}

ggplot(df_train, aes(gdp_per_capita, life_satisfaction)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F) +
  theme_classic()

```



## Natural regression spline

### Specify model

Specify the linear regression model:

```{r}

spline_spec <- 
  linear_reg() %>% 
  set_engine("lm")

```


We use the package [`recipe`](https://recipes.tidymodels.org) to prepare the data and tune the hyperparameter of the natural spline (`step_ns`): degrees of freedom (`deg_free`):

```{r}

spline_rec <-
  recipe(life_satisfaction ~ gdp_per_capita, data = df_train) %>%
  step_ns(gdp_per_capita, deg_free = tune("gdp_per_capita")) 

```


### Tuning

Note that we can use `parameters()` to detect and collect the parameters that have been flagged for tuning:

```{r}

parameters(spline_rec)

```

We use `update()` to tune the parameter objects:

```{r}

spline_param <-
  spline_rec %>%
    parameters() %>%
    update(
      gdp_per_capita = spline_degree()
           )

# Take a look at the tuning parameter
spline_degree()

```

### Train models

We use grid search ([`grid_max_entropy`](https://dials.tidymodels.org/reference/grid_max_entropy.html)) to test different spline hyperparameters:

```{r}

spline_grid <- grid_max_entropy(spline_param, 
                                size = 5)

```

Combine the linear regression model with the natural spline:

```{r}

spline_fit <- 
  tune_grid(spline_spec, # linear regression model
            spline_rec,  # our recipe
            resamples = cv_folds, # k-fold cross-validation 
            grid = spline_grid) # grid search with spline parameters

```

### Model evaluation

Show the best results according to RMSE:

```{r}

show_best(spline_fit, metric = "rmse")

```

The `.metrics` column has all of the holdout performance estimates for each parameter value:

```{r}

spline_fit$.metrics[[1]]

```

To get the average metric value for each parameter combination, `collect_metrics()` can be used. The values in the *mean* column are the averages of the k-fold resamples: 

```{r}

estimates <- collect_metrics(spline_fit)

estimates

```

The best RMSE values corresponded to:

```{r}

rmse_vals <-
  estimates %>%
  dplyr::filter(.metric == "rmse") %>%
  arrange(mean)

rmse_vals

```

Smaller degrees of freedom values correspond to more linear functions and the grid search indicates that more linearity is better. 

Relationship between the hyperparameter and RMSE:

```{r}

autoplot(spline_fit, metric = "rmse")

```


### Plot model

Visualize the model:

```{r}

library(splines)

ggplot(df_train, aes(gdp_per_capita, life_satisfaction)) + 
  geom_point() + 
  geom_smooth(method = "lm", formula = y ~ ns(x, 3), se = FALSE) +
  theme_classic()

```



## K-nearest neighbor

### Specify model

Recipe:

```{r}

knn_rec <- 
  recipe(life_satisfaction ~ gdp_per_capita, 
      data = df_train)

```


The number of neighbors and the distance weighting function are hyperparameters and will be optimized:

```{r}

knn_spec <-
  nearest_neighbor(neighbors = tune(), 
                   weight_func = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

```

Combine all steps in one workflow:

```{r}

knn_wflow <-
  workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)

```

### Training and tuning 


From this, the parameter set can be used to modify the range and values of parameters being optimized:

```{r}

knn_param <-
  knn_wflow %>%
  parameters() %>%
    update(
    neighbors = neighbors(c(3, 10)),
    weight_func = weight_func(values = c("rectangular", 
                                         "inv", 
                                         "gaussian", 
                                         "triangular"))
  )

```

This parameter collection can be passed to the grid functions via the `param_info` arguments.

#### Bayesian optimization

Instead of using grid search, an iterative method called *Bayesian optimization* can be used. This takes an initial set of results and tries to predict the next tuning parameters to evaluate.

Although no grid is required, the process requires a few additional pieces of information ([tidymodels documentation](https://tune.tidymodels.org/articles/getting_started.html#fnref2):

* A description of the search space. At a minimum, that would consist of ranges for numeric values and a list of values for categorical tuning parameters.

* An acquisition function that helps score potential tuning parameter values.

* A model for analyzing and making predictions of the best tuning parameter values. A Gaussian Process model is typical and used here.

The code to conduct the search is:

```{r}

ctrl <- control_bayes(verbose = TRUE)

set.seed(123)

knn_search <- tune_bayes(knn_wflow, 
                         resamples = cv_folds, 
                         initial = 2, 
                         iter = 4, 
                         param_info = knn_param, 
                         control = ctrl)

```


Note that we set `iter = 4` relatively low due to performanace reasons. Usually, we would use a higher number of iterations (e.g., 10). 

### Model evaluation

Visually, the performance gain was:

```{r}

autoplot(knn_search, 
         type = "performance", 
         metric = "rmse")

```

The best results here were:

```{r}

collect_metrics(knn_search) %>%
  filter(.metric == "rmse") %>%
  arrange(mean)

```

Best KNN model:

```{r}

knn_spec_best <-
  nearest_neighbor(neighbors = 5, 
                   weight_func = "triangular") %>%
  set_engine("kknn") %>%
  set_mode("regression")

knn_spec_best_fit <-
  knn_spec_best %>% 
  fit(life_satisfaction ~ gdp_per_capita, 
      data = df_train)

knn_spec_best_fit 

```



# Train final model

Final recipe for our model with the lowest RMSE:

```{r}

spline_rec_last <-
  recipe(life_satisfaction ~ gdp_per_capita, 
                data = df_train) %>%
  step_ns(gdp_per_capita, 
          deg_free = 3)  

```

Now, we have two options to fit the final model: without and with a workflow.

## Train without workflow

We can use the function `last_fit()` with our finalized model; this function first fits the finalized model on the full training data and evaluates the finalized model on the testing data.


```{r}

spline_final <- last_fit(spline_spec, 
                         spline_rec_last, 
                         split = df_split)

```

Performance measures:

```{r}

collect_metrics(spline_final)

```

Predictions:

```{r}

spline_final$.predictions

```

Instead of using this procedere, we could also use a workflow.

## Train with workflow

```{r}

spline_wflow_last <- 
  workflow() %>% 
  add_recipe(spline_rec_last) %>% 
  add_model(spline_spec) 

```

Last fit:

```{r}

spline_res <- 
  last_fit(spline_wflow_last,
         split = df_split)

```


Test set results:

```{r}

spline_res$.metrics[[1]]

```

Show workflow and coefficients:

```{r}

spline_res$.workflow

```

Predictions:

```{r}

spline_res$.predictions

```

You can also extract the test set predictions: 

```{r}

spline_res %>% collect_predictions()

```

Visualize the model:

```{r}

ggplot(df_train, aes(gdp_per_capita, life_satisfaction)) + 
  geom_point() + 
  geom_smooth(method = "lm", 
              formula = y ~ splines::ns(x, 3), se = F) +
  theme_classic()

```


# Evaluation

In this phase we would check whether our solution achieves the business objective. If this is the case, we can proceed to the last step of the data science lifecylce: deployment.


# Deplyoment

> Fitting and using the final model

The code in the previous chapter evaluated the final model trained on the training data using the testing data. Once we’ve determined the final model, we can train it on the full dataset and then use it to predict the response for new data.

## Train final model

Train final model for deployment:

```{r}

final_model <- fit(spline_wflow_last, df)

```

Take a look at the model:

```{r}

final_model

```


Visualize our final model:

```{r}

ggplot(df, aes(gdp_per_capita, life_satisfaction)) + 
  geom_point() + 
  geom_smooth(method = "lm", 
              formula = y ~ splines::bs(x, 3), se = F) +
  theme_classic()

```

Extract model (especially helpful if we use a workflow):

```{r}

model <- extract_model(final_model)

```

Now you are able to deploy the final model in your production infrastructure (e.g., as a web service).



## Prediction

Make a prediction for a new GDP value with the final model:

```{r}

X_new <-  tibble(
  gdp_per_capita = c(50000, 25000)
  )

(predict(final_model, new_data = X_new))

```





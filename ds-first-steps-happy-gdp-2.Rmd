---
title: "First steps in Data Science, Part II"
subtitle: "Does money make people happier?"
author: "Prof. Dr. Jan Kirenz, HdM Stuttgart"
output:
 html_document:
  code_download: true
  fig_height: 4
  fig_width: 4
  highlight: tango
  number_sections: yes
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  theme: paper
  df_print: paged
---


```{css, echo=FALSE}

/* css code to change the look of the HTML-output */
  
h1 {
  color: #D0313C;
  font-size: 200%;
  }
h2 {
  color: #D0313C;
  font-size: 150%;
  }
h3 {
  font-size: 120%;
  font-weight: bold;
  }
h4 {
  color: rgb(139, 142, 150);
  font-size: 100%;
  font-weight: bold;
  }

```



```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```


# Setup

```{r}

# Load packages
library(tidyverse) # collection of important data science packages
library(tidymodels)  # collection of packages for modeling and ml
library(janitor) # data preparation package
library(kknn) # K-nearest neighbor model

```

# Data import

```{r}

# Load the data from GitHub
LINK = "https://raw.githubusercontent.com/kirenz/datasets/master/oecd_gdp.csv"
df <- read_csv(LINK)

# Take a look at the data
glimpse(df)

```


```{r fig.width=4, fig.height=4}

# Change column names
df <-  clean_names(df)

# Visualize the data
ggplot(df, aes(x=gdp_per_capita, 
               y=life_satisfaction)) +
  geom_point() +
  theme_classic()

```


# Data splitting & preparation

First, we split the data: 

```{r}
set.seed(123)

df_split <- initial_split(df) 
df_train <- training(df_split) 
df_test <- testing(df_split)

```

We want to use three different models in this example:

1. simple linear regression.
2. natural spline in conjunction with a linear model,
3. K-nearest-neighbour model.

# Model selection and training

## K-fold cross-validation

```{r}
set.seed(123)

cv_folds <- vfold_cv(df_train, v = 5)

```


## Linear regression model

Specify the model:

```{r}

lm_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode(mode = "regression") 

```

Train the model:

```{r}

lm_fit <-
  lm_mod %>% 
  fit_resamples(life_satisfaction ~ gdp_per_capita, 
      resamples = cv_folds)

```

Model summary:

```{r}

# Performance measures for every fold
collect_metrics(lm_fit, summarize = FALSE)

# Average performance accross all folds
collect_metrics(lm_fit, summarize = TRUE)

```


## Natural regression spline

We use the package [`recipe`](https://recipes.tidymodels.org) to prepare the data and tune the model hyperparamters. The main idea of the recipes package is to define a recipe ("rec") or blueprint that can be used to sequentially define the encodings and preprocessing of the data (i.e. “feature engineering”):

```{r}

df_rec <-
  recipe(life_satisfaction ~ gdp_per_capita, 
                data = df_train) %>%
  step_ns(gdp_per_capita, 
          deg_free = tune("gdp_per_capita")) # tuning parmeter for model 2

```

We can use `parameters()`to detect and collect the parameters that have been flagged for tuning:

```{r}

parameters(df_rec)

```

Use `update()` to tune parameter objects:

```{r}

spline_param <-
  df_rec %>%
    parameters() %>%
    update(gdp_per_capita = spline_degree())

# Take a look at the tuning parameters
spline_param

```

Grid search:

```{r}

# Create a grid
spline_grid <- grid_max_entropy(spline_param, 
                                size = 5)

```

Linear regression model:

```{r}

lm_mod_sp <- 
  linear_reg() %>% 
  set_engine("lm")

```

Combine the linear regression model with a natural spline:

```{r}

spline_res <- 
  tune_grid(lm_mod_sp, # linear regression
            df_rec,  # recipe
            resamples = cv_folds, # k-fold-crossvalidation 
            grid = spline_grid) # grid search

spline_res

```


Show the best results according to RMSE:

```{r}

show_best(spline_res, metric = "rmse")

```

As an example, here are the performance metrics for the first parameter combination:

```{r}
spline_res$.metrics[[1]]
```


Average metric value for each parameter combination

```{r}
estimates <- collect_metrics(spline_res)
estimates
```

The values in the mean column are the averages of the 10 resamples. The best RMSE values corresponded to:

```{r}

rmse_vals <-
  estimates %>%
  dplyr::filter(.metric == "rmse") %>%
  arrange(mean)

rmse_vals

```

Smaller degrees of freedom values correspond to more linear functions, but the grid search indicates that more nonlinearity is better. 

What was the relationship between these two parameters and RMSE?

```{r}

autoplot(spline_res, metric = "rmse")

```


## K-nearest neighbor

The number of neighbors and the distance weighting function will be optimized:

```{r}

knn_mod <-
  nearest_neighbor(neighbors = tune(), 
                   weight_func = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

```

The easiest approach to optimize the pre-processing and model parameters is to bundle these objects into a workflow:

```{r}
library(workflows)

knn_wflow <-
  workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(df_rec)

```

From this, the parameter set can be used to modify the range and values of parameters being optimized3:

```{r}

knn_param <-
  knn_wflow %>%
  parameters() %>%
    update(
    gdp_per_capita = spline_degree(c(2, 5)),
    neighbors = neighbors(c(3, 10)),
    weight_func = weight_func(values = c("rectangular", 
                                         "inv", 
                                         "gaussian", 
                                         "triangular"))
  )

```

This parameter collection can be passed to the grid functions via the param_info arguments.

Instead of using grid search, an iterative method called Bayesian optimization can be used. This takes an initial set of results and tries to predict the next tuning parameters to evaluate.

Although no grid is required, the process requires a few additional pieces of information:

* A description of the search space. At a minimum, the would consist of ranges for numeric values and a list of values for categorical tuning parameters.

* An acquisition function that helps score potential tuning parameter values.

* A model for analyzing and making predictions of the best tuning parameter values. A Gaussian Process model is typical and used here.

The code to conduct the search is:

```{r}

ctrl <- control_bayes(verbose = TRUE)

set.seed(8154)

knn_search <- tune_bayes(knn_wflow, 
                         resamples = cv_folds, 
                         initial = 5, 
                         iter = 10,
                         param_info = knn_param, 
                         control = ctrl)

```

Visually, the performance gain was:

```{r}

autoplot(knn_search, type = "performance", metric = "rmse")

```

The best results here were:

```{r}

collect_metrics(knn_search) %>%
  dplyr::filter(.metric == "rmse") %>%
  arrange(mean)

```

With this intrinsically nonlinear model there is less reliance on the nonlinear terms created by the recipe.


# Train final model

Train best model with all training data:

```{r}

last_lm_fit <- 
  lm_mod %>% 
  fit(life_satisfaction ~ gdp_per_capita, 
                data = df_train)

```

Model coefficients:

```{r}

tidy(last_lm_fit)

```

# Model testing

Predictions:

```{r}

last_lm_pred <-
  last_lm_fit %>%
  predict(new_data = df_test) %>%
  mutate(y_truth = df_test$life_satisfaction)

```

Root mean squared error:

```{r}

rmse(truth = y_truth, 
     estimate = .pred, 
     last_lm_pred) 

```

More performance measures:

```{r}

glance(last_lm_fit$fit)

```

Make a prediction for a new GDP value:

```{r}

X_new <-  tibble(gdp_per_capita = c(50000))

```

```{r}

(predict(last_lm_fit, new_data = X_new))

```

